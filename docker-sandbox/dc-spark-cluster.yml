# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.5'
services:
  #
  # HDFS & YARN Sandbox
  #
  namenode:
    image: brijeshdhaker/hadoop-namenode:3.3.4
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - sandbox_hadoop_dfs_name:/hadoop/dfs/name
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
    environment:
      - CLUSTER_NAME=docker-sandbox
    env_file:
      - envs/docker_hadoop.env

  #
  datanode:
    image: brijeshdhaker/hadoop-datanode:3.3.4
    container_name: datanode
    hostname: datanode
    ports:
      - "9864:9864"
      - "9866:9866"
    restart: always
    volumes:
      - sandbox_hadoop_dfs_data:/hadoop/dfs/data
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - envs/docker_hadoop.env

  #
  #
  #
  spark-master:
    image: docker.io/brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.3.4
      - sandbox_hive:/opt/hive-3.1.2
    environment:
      - SPARK_MASTER_LOG=/opt/spark/logs/spark-master.out
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_DRIVER_CORES=1
      - SPARK_DRIVER_MEMORY=1G

  #
  #
  #
  spark-worker-a:
    image: docker.io/brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-worker-a
    hostname: spark-worker-a
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-a
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_LOG=/opt/spark/logs/spark-worker.out
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.3.4
      - sandbox_hive:/opt/hive-3.1.2


  #
  #
  #
  spark-worker-b:
    image: docker.io/brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-worker-b
    hostname: spark-worker-b
    ports:
      - "8082:8082"
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-b
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_WORKER_LOG=/opt/spark/logs/spark-worker.out
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.3.4
      - sandbox_hive:/opt/hive-3.1.2

  #
  #
  #
  spark-historyserver:
    image: docker.io/brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-historyserver
    hostname: spark-historyserver
    environment:
      SPARK_WORKLOAD: HistoryServer
      SPARK_HISTORY_CONF_FILE: '/opt/spark/conf/spark-defaults.conf'
    depends_on:
      - namenode
      - datanode
    ports:
      - "18080:18080"
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

  #
  # Zeppelin Notebook
  #
  zeppelin:
    image: apache/zeppelin:0.10.1
    container_name: zeppelin
    hostname: zeppelin
    env_file:
      - envs/docker_zeppelin.env
    ports:
      - "9080:8080"
      - "4040:4040"
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.3.4
      - sandbox_hive:/opt/hive-3.1.2
      - sandbox_hbase:/opt/hbase-2.4.9
      - sandbox_hbase_client:/opt/hbase-client
      - sandbox_spark:/opt/spark-3.1.2
      - sandbox_zeppelin:/opt/zeppelin
      - sandbox_zeppelin_notebook:/opt/notebook


#
volumes:
  sandbox_host_path:
    external: true
  sandbox_base_path:
    external: true
  sandbox_hadoop_data:
    external: true
  sandbox_hadoop_dfs_name:
    external: true
  sandbox_hadoop_dfs_data:
    external: true
  sandbox_yarn_history:
    external: true
  sandbox_zeppelin:
    external: true
  sandbox_zeppelin_notebook:
    external: true
  sandbox_hadoop:
    external: true
  sandbox_hbase:
    external: true
  sandbox_hbase_client:
    external: true
  sandbox_hive:
    external: true
  sandbox_spark:
    external: true

#
networks:
  default:
    external: true
    driver: bridge
    name: sandbox-bigdata.net
