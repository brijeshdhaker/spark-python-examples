# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.5'
services:
  #
  # HDFS & YARN Sandbox
  #
  namenode:
    image: brijeshdhaker/hadoop-namenode:3.2.1
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - sandbox_hadoop_dfs_name:/hadoop/dfs/name
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
    environment:
      - CLUSTER_NAME=docker-sandbox
    env_file:
      - sandbox3.env

  #
  datanode:
    image: brijeshdhaker/hadoop-datanode:3.2.1
    container_name: datanode
    hostname: datanode
    ports:
      - "9864:9864"
      - "9866:9866"
    restart: always
    volumes:
      - sandbox_hadoop_dfs_data:/hadoop/dfs/data
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - sandbox3.env

  #
  spark-master:
    image: brijeshdhaker/spark-master:3.1.2
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    env_file:
      - sandbox3.env
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.2.1
      - sandbox_hive:/opt/hive-3.1.2
    environment:
      WEBUI_PORT: 8080

  ##
  spark-worker01:
    image: brijeshdhaker/spark-worker:3.1.2
    container_name: spark-worker01
    hostname: spark-worker01
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    env_file:
      - sandbox3.env
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.2.1
      - sandbox_hive:/opt/hive-3.1.2
    environment:
      WORKER_WEBUI_PORT: 8081

  #
  spark-worker02:
    image: brijeshdhaker/spark-worker:3.1.2
    container_name: spark-worker02
    hostname: spark-worker02
    ports:
      - "8082:8082"
    depends_on:
      - spark-master
    env_file:
      - sandbox3.env
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.2.1
      - sandbox_hive:/opt/hive-3.1.2
    environment:
      WORKER_WEBUI_PORT: 8082

  #  #
  #  spark-worker03:
  #    image: brijeshdhaker/spark-worker:3.1.2
  #    container_name: spark-worker03
  #    hostname: spark-worker03
  #    ports:
  #      - "8083:8083"
  #    depends_on:
  #      - spark-master
  #    env_file:
  #      - sandbox3.env
  #    volumes:
  #        - sandbox_host_path:/apps/hostpath
  #        - sandbox_hadoop:/opt/hadoop-3.2.1
  #        - sandbox_hive:/opt/hive-3.1.2
  #    environment:
  #      WORKER_WEBUI_PORT: 8083

  #
  #
  #
  spark-historyserver:
    image: brijeshdhaker/spark-historyserver:3.1.2
    container_name: spark-historyserver
    hostname: spark-historyserver
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864"
    depends_on:
      - namenode
      - datanode
    ports:
      - "18080:18080"
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.2.1
      - sandbox_hive:/opt/hive-3.1.2

  #
  # Zeppelin Notebook
  #
  zeppelin:
    image: apache/zeppelin:0.10.1
    container_name: zeppelin
    hostname: zeppelin
    env_file:
      - sandbox3.env
    ports:
      - "9080:8080"
      - "4040:4040"
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop:/opt/hadoop-3.2.1
      - sandbox_hive:/opt/hive-3.1.2
      - sandbox_hbase:/opt/hbase-2.4.9
      - sandbox_hbase_client:/opt/hbase-client
      - sandbox_spark:/opt/spark
#      - sandbox_zeppelin:/opt/zeppelin


#
volumes:
  sandbox_host_path:
    external: true
  sandbox_base_path:
    external: true
  sandbox_m2:
    external: true
  sandbox_ivy2:
    external: true
  sandbox_hadoop_data:
    external: true
  sandbox_hadoop_dfs_name:
    external: true
  sandbox_hadoop_dfs_data:
    external: true
  sandbox_yarn_history:
    external: true
  sandbox_postgres:
    external: true
  sandbox_zeppelin:
    external: true
  sandbox_zeppelin_conf:
    external: true
  sandbox_hadoop:
    external: true
  sandbox_hbase:
    external: true
  sandbox_hbase_client:
    external: true
  sandbox_hive:
    external: true
  sandbox_spark:
    external: true

#
networks:
  default:
    external: true
    driver: bridge
    name: sandbox-bigdata.net
