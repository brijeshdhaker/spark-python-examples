# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.5'
services:
  #
  #
  #
  spark-master:
    image: docker.io/brijeshdhaker/spark-standalon:3.1.2
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive
    environment:
      - SPARK_MASTER_LOG=/opt/spark/logs/spark-master.out
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_DRIVER_CORES=1
      - SPARK_DRIVER_MEMORY=1G

  #
  #
  #
  spark-worker-a:
    image: docker.io/brijeshdhaker/spark-standalon:3.1.2
    container_name: spark-worker-a
    hostname: spark-worker-a
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-a
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_LOG=/opt/spark/logs/spark-worker.out
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive


  #
  #
  #
  spark-worker-b:
    image: docker.io/brijeshdhaker/spark-standalon:3.1.2
    container_name: spark-worker-b
    hostname: spark-worker-b
    ports:
      - "8082:8082"
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-b
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_WORKER_LOG=/opt/spark/logs/spark-worker.out
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive

  #
  #
  #
  spark-historyserver:
    image: docker.io/brijeshdhaker/spark-standalon:3.1.2
    container_name: spark-historyserver
    hostname: spark-historyserver
    environment:
      SPARK_WORKLOAD: HistoryServer
      SPARK_HISTORY_CONF_FILE: '/opt/spark/conf/spark-defaults.conf'
    ports:
      - "18080:18080"
    volumes:
      - sandbox_apps_path:/apps
      - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

  #
  # Zeppelin Notebook
  #
  zeppelin:
    image: apache/zeppelin:0.10.1
    container_name: zeppelin
    hostname: zeppelin
    env_file:
      - envs/docker_zeppelin.env
    ports:
      - "9080:8080"
      - "4040:4040"
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - sandbox_hbase_246:/opt/hbase
      - sandbox_hbase_117:/opt/hbase-client
      - sandbox_spark_312:/opt/spark
      - sandbox_zeppelin:/opt/zeppelin
      - sandbox_zeppelin_notebook:/opt/notebook


#
volumes:
  sandbox_apps_path:
    external: true
  #
  sandbox_hadoop_311:
    external: true
  sandbox_hadoop_data:
    external: true
  sandbox_hadoop_dfs_name:
    external: true
  sandbox_hadoop_dfs_data:
    external: true
  sandbox_yarn_history:
    external: true
  sandbox_zeppelin:
    external: true
  sandbox_zeppelin_notebook:
    external: true
  sandbox_hadoop:
    external: true
  #
  sandbox_hbase_246:
    external: true
  sandbox_hbase_117:
    external: true
  #
  sandbox_hive_313:
    external: true
  sandbox_spark_312:
    external: true

#
networks:
  default:
    external: true
    driver: bridge
    name: sandbox.net
