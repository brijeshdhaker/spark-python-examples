{"paragraphs":[{"text":"%spark.conf\n\n#\n#\n# SPARK_HOME /opt/spark-3.1.2\n\nSPARK_HOME /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark\n\n#\n# \n# set spark execution mode\n# Please dont set deployment in interpreter settings.\n#\n\nmaster local[*]\n\n#\n# spark.jars can be used for adding any local jar files into spark interpreter\n#\n#spark.jars  file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n#spark.executor.extraClassPath file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n#spark.executor.extraLibrary file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n\n\n#\n# set driver memory to 512M\n#\n\nspark.driver.memory 512M\n\n#\n# set executor number to be 2\n#\n\nspark.executor.instances  2\n\n\n#\n# set executor memory 512M\n#\n\nspark.executor.memory  512M","user":"anonymous","dateUpdated":"2021-12-11T21:52:57+0530","progress":0,"config":{"editorSetting":{"language":"text","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/text","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677898_1767557535","id":"paragraph_1638439432008_394136546","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","focus":false,"dateFinished":"2021-12-11T21:52:57+0530","dateStarted":"2021-12-11T21:52:57+0530"},{"text":"%pyspark\n\n#\n# MAP Transformation : map(func) - Return a new distributed dataset formed by passing each element of the source through a function func.\n#\n\nmapdata = [('A', 1), ('A', 2), ('A', 3), ('A', 4), ('A', 5), ('A', 6), ('A', 7), ('A', 8), ('A', 9), ('A', 10)]\nmapRDD = sc.parallelize(mapdata,4)\n\nprint(mapRDD.glom().collect())\n\n\nmapResultRDD = mapRDD.map(lambda x : (x[0], x[1]*x[1]))\nprint(mapResultRDD.glom().collect())","user":"anonymous","dateUpdated":"2021-12-11T21:53:02+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[[('A', 1), ('A', 2)], [('A', 3), ('A', 4)], [('A', 5), ('A', 6)], [('A', 7), ('A', 8), ('A', 9), ('A', 10)]]\n[[('A', 1), ('A', 4)], [('A', 9), ('A', 16)], [('A', 25), ('A', 36)], [('A', 49), ('A', 64), ('A', 81), ('A', 100)]]\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=0"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=1"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677898_779938943","id":"paragraph_1638967416834_2141960040","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:53:27+0530","dateStarted":"2021-12-11T21:53:02+0530"},{"text":"%pyspark\n\n#\n# Filter Transformation : filter(func)\t- Return a new dataset formed by selecting those elements of the source on which func returns true.\n#\n\nfilterdata = [('A', 1), ('A', 2), ('A', 3), ('A', 4), ('A', 5), ('A', 6), ('A', 7), ('A', 8), ('A', 9), ('A', 10)]\nfilterRDD = sc.parallelize(filterdata,4)\n\nprint(filterRDD.glom().collect())\n\n\nfilterResultRDD = filterRDD.filter(lambda x : x[1] >= 5)\nprint(filterResultRDD.glom().collect())","user":"anonymous","dateUpdated":"2021-12-11T21:53:50+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[[('A', 1), ('A', 2)], [('A', 3), ('A', 4)], [('A', 5), ('A', 6)], [('A', 7), ('A', 8), ('A', 9), ('A', 10)]]\n[[], [], [('A', 5), ('A', 6)], [('A', 7), ('A', 8), ('A', 9), ('A', 10)]]\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=2"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=3"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677898_1954609421","id":"paragraph_1638968029858_1047212913","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:53:51+0530","dateStarted":"2021-12-11T21:53:50+0530"},{"text":"%pyspark\n\n#\n# flatMap Transformation : flatMap(func) - Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n#\n\nflatMapdata = [('A', 1), ('B', 2), ('C', 3), ('D', 4), ('E', 5), ('F', 6), ('G', 7), ('H', 8), ('I', 9), ('J', 10)]\nflatMapRDD = sc.parallelize(flatMapdata,4)\n\nprint(flatMapRDD.glom().collect())\n\ndef convertorR(x) :\n    i = 0\n    collections = []\n    while i < x[1] :\n        collections.append((x[0], x[1]))\n        i = i + 1\n    \n    return collections\n    \n        \ndef convertorY(x) :\n    i = 0\n    while i < x[1] :\n        yield((x[0], x[1]))\n        i = i + 1\n    \n    \nflatMapResultRDD = flatMapRDD.flatMap(convertorY)\nprint(flatMapResultRDD.glom().collect())\n","user":"anonymous","dateUpdated":"2021-12-11T21:55:10+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[[('A', 1), ('B', 2)], [('C', 3), ('D', 4)], [('E', 5), ('F', 6)], [('G', 7), ('H', 8), ('I', 9), ('J', 10)]]\n[[('A', 1), ('B', 2), ('B', 2)], [('C', 3), ('C', 3), ('C', 3), ('D', 4), ('D', 4), ('D', 4), ('D', 4)], [('E', 5), ('E', 5), ('E', 5), ('E', 5), ('E', 5), ('F', 6), ('F', 6), ('F', 6), ('F', 6), ('F', 6), ('F', 6)], [('G', 7), ('G', 7), ('G', 7), ('G', 7), ('G', 7), ('G', 7), ('G', 7), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10)]]\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=6"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=7"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677898_130620317","id":"paragraph_1638968285768_1397297707","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:55:11+0530","dateStarted":"2021-12-11T21:55:10+0530","focus":false},{"text":"%pyspark\n\n#\n# Sample : sample(withReplacement, fraction, seed)\t- Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.\n#\n\nrdd_sample1 = flatMapResultRDD.sample(False, .2, 4)\nfor e in rdd_sample1.collect() : print(e)\n\n#print(rdd_sample1.glom().collect())\n\n#rdd_sample2 = flatMapResultRDD.sample(False, .4, 4)\n#print(rdd_sample2.glom().collect())","user":"anonymous","dateUpdated":"2021-12-11T21:56:32+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('B', 2)\n('C', 3)\n('D', 4)\n('F', 6)\n('G', 7)\n('G', 7)\n('G', 7)\n('H', 8)\n('H', 8)\n('I', 9)\n('I', 9)\n('I', 9)\n('J', 10)\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=8"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_633178705","id":"paragraph_1638970913522_1169300364","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:56:32+0530","dateStarted":"2021-12-11T21:56:32+0530","focus":false},{"text":"%pyspark\n\n#\n# Reduce Example\n#\n\ndef add(x, y):\n    print('{}+{}={}'.format(x, y, x+y))\n    return x+y\n    \n    \ndata = [1,2,3,4,5]\n\nrdd_1 = spark.sparkContext.parallelize(data,2)\n\nprint(\"Partitions : {}\".format(rdd_1.getNumPartitions()))\nprint(\"Total : {}\".format(rdd_1.reduce(add)))\nprint(\"Total with init value : {}\".format(rdd_1.reduce(lambda x, y : x + y)))\nprint(\"Min : {}\".format(rdd_1.reduce(lambda x, y : min(x, y) )))\nprint(\"Max : {}\".format(rdd_1.reduce(lambda x, y : max(x, y) )))","user":"anonymous","dateUpdated":"2021-12-11T21:57:20+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Partitions : 2\n3+12=15\nTotal : 15\nTotal with init value : 15\nMin : 1\nMax : 5\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=9"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=10"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=11"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=12"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_2073862935","id":"paragraph_1638440185623_41423979","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:57:22+0530","dateStarted":"2021-12-11T21:57:20+0530"},{"text":"%pyspark\n\n#\n# fold with 0 Example\n#\ndata = [1,2,3,4,5]\n\nrdd_1 = spark.sparkContext.parallelize(data, 2)\n\nprint(\"Partitions : {}\".format(rdd_1.getNumPartitions()))\nprint(\"Total : {}\".format(rdd_1.fold(0, lambda x, y : x + y)))\nprint(\"Total with init value 2 : {}\".format(rdd_1.fold(0, lambda x, y : x + y)))\nprint(\"Min : {}\".format(rdd_1.fold(0, lambda x, y : min(x, y) )))\nprint(\"Max : {}\".format(rdd_1.fold(0, lambda x, y : max(x, y) )))\n","user":"anonymous","dateUpdated":"2021-12-11T21:57:26+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Partitions : 2\nTotal : 15\nTotal with init value 2 : 15\nMin : 0\nMax : 5\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=13"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=14"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=15"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=16"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_1150286036","id":"paragraph_1638440302410_2060859008","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:57:26+0530","dateStarted":"2021-12-11T21:57:26+0530"},{"text":"%pyspark\n\n#\n# fold with 2 Example\n#\ndata = [1,2,3,4,5]\n\nrdd_1 = spark.sparkContext.parallelize(data, 2)\n\nprint(\"Partitions : {}\".format(rdd_1.getNumPartitions()))\nprint(\"Total : {}\".format(rdd_1.fold(2, add)))\nprint(\"Total with init value 2 : {}\".format(rdd_1.fold(2, lambda x, y : x + y)))\nprint(\"Min : {}\".format(rdd_1.fold(2, lambda x, y : min(x, y) )))\nprint(\"Max : {}\".format(rdd_1.fold(2, lambda x, y : max(x, y) )))\n\n","user":"anonymous","dateUpdated":"2021-12-11T21:51:17+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Partitions : 2\n2+5=7\n7+14=21\nTotal : 21\nTotal with init value 2 : 21\nMin : 1\nMax : 5\n"}]},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_919255609","id":"paragraph_1638962626048_985387068","dateCreated":"2021-12-11T21:51:17+0530","status":"READY"},{"text":"%pyspark\n\n#\n# Broadcast Variables\n#\n\nbroadcastVar = sc.broadcast([1, 2, 3])\nprint(\"{}\".format(broadcastVar.value))\n\n","user":"anonymous","dateUpdated":"2021-12-11T21:58:04+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 2, 3]\n"}]},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_794086011","id":"paragraph_1638962691917_1511648622","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:58:04+0530","dateStarted":"2021-12-11T21:58:04+0530"},{"text":"%pyspark\n\n#\n# Accumulators\n#\n\naccum = sc.accumulator(0)\n\nacc_rd = sc.parallelize(data,4)\nacc_rd.foreach(lambda x : accum.add(x))\n\nprint(\"{}\".format(accum.value))\n\n","user":"anonymous","dateUpdated":"2021-12-11T21:58:07+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"15\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=17"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_1797061646","id":"paragraph_1638960380810_818547062","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T21:58:07+0530","dateStarted":"2021-12-11T21:58:07+0530"},{"text":"%pyspark\n\n#\n#\n#\n\npremierRDD = sc.parallelize([\n (\"Arsenal\", \"2014–2015\", 75), (\"Arsenal\", \"2015–2016\", 71), (\"Arsenal\", \"2016–2017\", 75), (\"Arsenal\", \"2017–2018\", 63),\n (\"Chelsea\", \"2014–2015\", 87), (\"Chelsea\", \"2015–2016\", 50), (\"Chelsea\", \"2016–2017\", 93), (\"Chelsea\", \"2017–2018\", 70), \n (\"Liverpool\", \"2014–2015\", 62), (\"Liverpool\", \"2015–2016\", 60), (\"Liverpool\", \"2016–2017\", 76), (\"Liverpool\", \"2017–2018\", 75),\n (\"M. City\", \"2014–2015\", 79), (\"M. City\", \"2015–2016\", 66), (\"M. City\", \"2016–2017\", 78), (\"M. City\", \"2017–2018\", 100), \n (\"M. United\", \"2014–2015\", 70), (\"M. United\", \"2015–2016\", 66), (\"M. United\", \"2016–2017\", 69), (\"M. United\", \"2017–2018\", 81) \n ])\n\n# for i in premierRDD.collect(): print(i)\n\n\n#\n#\n#\npremierMap = premierRDD.map(lambda t: (t[0], (t[1], t[2])))\npremierMap.first()\n\n\n#\n# find the maximum points of each teams.\n#\n\n#\nprint(\" Max\")\nprint(\" #\" * 30)\ndef seqFunc(acc, teams):\n if(acc > teams[1]):\n    return acc \n else: \n    return teams[1]\n\n#\ndef combFunc(acc1, acc2):\n if(acc1 > acc2):\n    return acc1 \n else:\n    return acc2\n    \npremierMax = premierMap.aggregateByKey(0, seqFunc, combFunc)\nfor i in premierMax.collect(): print(i)\n\nprint(\" #\" * 30)\n\n#\n# Let’s find total points and then start to describe zeroValue\n#\nprint(\"  \" * 30)\nprint(\" Aggregate Sum\")\nprint(\" #\" * 30)\n\ntotalSeqFunc = (lambda x, y: (x[0] + y[1], x[1] + 1))\ntotalCombFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n\npremierAgg = premierMap.aggregateByKey((0,0), totalSeqFunc, totalCombFunc)\nfor i in premierAgg.collect(): print(i)\n\n","user":"anonymous","dateUpdated":"2021-12-11T22:02:00+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":" Max\n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n('Chelsea', 93)\n('Arsenal', 75)\n('M. City', 100)\n('Liverpool', 76)\n('M. United', 81)\n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n                                                            \n Aggregate Sum\n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n('Chelsea', (300, 4))\n('Arsenal', (284, 4))\n('M. City', (323, 4))\n('Liverpool', (273, 4))\n('M. United', (286, 4))\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=33"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=34"},{"jobUrl":"http://quickstart-bigdata:4040/jobs/job?id=35"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_2022607017","id":"paragraph_1638960664997_1447366215","dateCreated":"2021-12-11T21:51:17+0530","status":"FINISHED","dateFinished":"2021-12-11T22:02:01+0530","dateStarted":"2021-12-11T22:02:00+0530","focus":false},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-12-11T21:51:17+0530","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1639239677899_1333903264","id":"paragraph_1639021076440_359626603","dateCreated":"2021-12-11T21:51:17+0530","status":"READY"}],"name":"pyspark-local-rdd-actions","id":"2GP9VDXTA","defaultInterpreterGroup":"spark","version":"0.10.0","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{},"path":"/pyspark-local-rdd-actions"}