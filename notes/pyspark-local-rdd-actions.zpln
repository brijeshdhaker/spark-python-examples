{
  "paragraphs": [
    {
      "text": "%spark.conf\n\n#\n#\n# SPARK_HOME /opt/cloudera/parcels/CDH/lib/spark\n\nSPARK_HOME /opt/spark-3.1.2\n\n#\n# \n# set spark execution mode\n# Please dont set deployment in interpreter settings.\n#\n\nmaster local[*]\n\n#\n# spark.jars can be used for adding any local jar files into spark interpreter\n#\n#spark.jars  file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n#spark.executor.extraClassPath file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n#spark.executor.extraLibrary file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n\n\n#\n# set driver memory to 512M\n#\n\nspark.driver.memory 512M\n\n#\n# set executor number to be 2\n#\n\nspark.executor.instances  2\n\n\n#\n# set executor memory 512M\n#\n\nspark.executor.memory  512M",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T13:18:00+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638959080602_379625872",
      "id": "paragraph_1638439432008_394136546",
      "dateCreated": "2021-12-08T10:24:40+0000",
      "dateStarted": "2021-12-08T13:18:00+0000",
      "dateFinished": "2021-12-08T13:18:00+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:186"
    },
    {
      "text": "%pyspark\n\n#\n# MAP Transformation : map(func) - Return a new distributed dataset formed by passing each element of the source through a function func.\n#\n\nmapdata = [('A', 1), ('A', 2), ('A', 3), ('A', 4), ('A', 5), ('A', 6), ('A', 7), ('A', 8), ('A', 9), ('A', 10)]\nmapRDD = sc.parallelize(mapdata,4)\n\nprint(mapRDD.glom().collect())\n\n\nmapResultRDD = mapRDD.map(lambda x : (x[0], x[1]*x[1]))\nprint(mapResultRDD.glom().collect())",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T12:52:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[[('A', 1), ('A', 2)], [('A', 3), ('A', 4)], [('A', 5), ('A', 6)], [('A', 7), ('A', 8), ('A', 9), ('A', 10)]]\n[[('A', 1), ('A', 4)], [('A', 9), ('A', 16)], [('A', 25), ('A', 36)], [('A', 49), ('A', 64), ('A', 81), ('A', 100)]]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=16",
              "$$hashKey": "object:567"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=17",
              "$$hashKey": "object:568"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638967416834_2141960040",
      "id": "paragraph_1638967416834_2141960040",
      "dateCreated": "2021-12-08T12:43:36+0000",
      "dateStarted": "2021-12-08T12:51:53+0000",
      "dateFinished": "2021-12-08T12:51:53+0000",
      "status": "FINISHED",
      "$$hashKey": "object:187"
    },
    {
      "text": "%pyspark\n\n#\n# Filter Transformation : filter(func)\t- Return a new dataset formed by selecting those elements of the source on which func returns true.\n#\n\nfilterdata = [('A', 1), ('A', 2), ('A', 3), ('A', 4), ('A', 5), ('A', 6), ('A', 7), ('A', 8), ('A', 9), ('A', 10)]\nfilterRDD = sc.parallelize(filterdata,4)\n\nprint(filterRDD.glom().collect())\n\n\nfilterResultRDD = filterRDD.filter(lambda x : x[1] >= 5)\nprint(filterResultRDD.glom().collect())",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T12:57:16+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[[('A', 1), ('A', 2)], [('A', 3), ('A', 4)], [('A', 5), ('A', 6)], [('A', 7), ('A', 8), ('A', 9), ('A', 10)]]\n[[], [], [('A', 5), ('A', 6)], [('A', 7), ('A', 8), ('A', 9), ('A', 10)]]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=22",
              "$$hashKey": "object:618"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=23",
              "$$hashKey": "object:619"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638968029858_1047212913",
      "id": "paragraph_1638968029858_1047212913",
      "dateCreated": "2021-12-08T12:53:49+0000",
      "dateStarted": "2021-12-08T12:57:16+0000",
      "dateFinished": "2021-12-08T12:57:16+0000",
      "status": "FINISHED",
      "$$hashKey": "object:188"
    },
    {
      "text": "%pyspark\n\n#\n# flatMap Transformation : flatMap(func) - Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n#\n\nflatMapdata = [('A', 1), ('B', 2), ('C', 3), ('D', 4), ('E', 5), ('F', 6), ('G', 7), ('H', 8), ('I', 9), ('J', 10)]\nflatMapRDD = sc.parallelize(flatMapdata,4)\n\nprint(flatMapRDD.glom().collect())\n\ndef convertorR(x) :\n    i = 0\n    collections = []\n    while i < x[1] :\n        collections.append((x[0], x[1]))\n        i = i + 1\n    \n    return collections\n    \n        \ndef convertorY(x) :\n    i = 0\n    while i < x[1] :\n        yield((x[0], x[1]))\n        i = i + 1\n    \n    \n    \n    \nflatMapResultRDD = flatMapRDD.flatMap(convertorY)\nprint(flatMapResultRDD.glom().collect())\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T13:21:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[[('A', 1), ('B', 2)], [('C', 3), ('D', 4)], [('E', 5), ('F', 6)], [('G', 7), ('H', 8), ('I', 9), ('J', 10)]]\n[[('A', 1), ('B', 2), ('B', 2)], [('C', 3), ('C', 3), ('C', 3), ('D', 4), ('D', 4), ('D', 4), ('D', 4)], [('E', 5), ('E', 5), ('E', 5), ('E', 5), ('E', 5), ('F', 6), ('F', 6), ('F', 6), ('F', 6), ('F', 6), ('F', 6)], [('G', 7), ('G', 7), ('G', 7), ('G', 7), ('G', 7), ('G', 7), ('G', 7), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('H', 8), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('I', 9), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10), ('J', 10)]]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=4",
              "$$hashKey": "object:669"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=5",
              "$$hashKey": "object:670"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638968285768_1397297707",
      "id": "paragraph_1638968285768_1397297707",
      "dateCreated": "2021-12-08T12:58:05+0000",
      "dateStarted": "2021-12-08T13:20:37+0000",
      "dateFinished": "2021-12-08T13:20:38+0000",
      "status": "FINISHED",
      "$$hashKey": "object:189"
    },
    {
      "text": "%pyspark\n\n#\n# Sample : sample(withReplacement, fraction, seed)\t- Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.\n#\n\nrdd_sample1 = flatMapResultRDD.sample(False, .2, 4)\nprint(rdd_sample1.collect())\nrdd_sample1.foreach(lambda x : print(x[1]))\n\n#print(rdd_sample1.glom().collect())\n\n#rdd_sample2 = flatMapResultRDD.sample(False, .4, 4)\n#print(rdd_sample2.glom().collect())",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T13:51:45+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[('C', 3), ('D', 4), ('F', 6), ('G', 7), ('G', 7), ('H', 8), ('H', 8), ('H', 8), ('I', 9), ('I', 9), ('I', 9)]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=22",
              "$$hashKey": "object:720"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=23",
              "$$hashKey": "object:721"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638970913522_1169300364",
      "id": "paragraph_1638970913522_1169300364",
      "dateCreated": "2021-12-08T13:41:53+0000",
      "dateStarted": "2021-12-08T13:51:45+0000",
      "dateFinished": "2021-12-08T13:51:45+0000",
      "status": "FINISHED",
      "$$hashKey": "object:190"
    },
    {
      "text": "%pyspark\n\n#\n# Reduce Example\n#\n\ndef add(x, y):\n    print('{}+{}={}'.format(x, y, x+y))\n    return x+y\n    \n    \ndata = [1,2,3,4,5]\n\nrdd_1 = spark.sparkContext.parallelize(data,2)\n\nprint(\"Partitions : {}\".format(rdd_1.getNumPartitions()))\nprint(\"Total : {}\".format(rdd_1.reduce(add)))\nprint(\"Total with init value : {}\".format(rdd_1.reduce(lambda x, y : x + y)))\nprint(\"Min : {}\".format(rdd_1.reduce(lambda x, y : min(x, y) )))\nprint(\"Max : {}\".format(rdd_1.reduce(lambda x, y : max(x, y) )))",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T10:28:41+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Partitions : 2\n3+4=71+2=3\n\n7+5=12\n3+12=15\nTotal : 15\nTotal with init value : 15\nMin : 1\nMax : 5\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=0",
              "$$hashKey": "object:771"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=1",
              "$$hashKey": "object:772"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=2",
              "$$hashKey": "object:773"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=3",
              "$$hashKey": "object:774"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638959080603_1669088003",
      "id": "paragraph_1638440185623_41423979",
      "dateCreated": "2021-12-08T10:24:40+0000",
      "dateStarted": "2021-12-08T10:28:41+0000",
      "dateFinished": "2021-12-08T10:28:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:191"
    },
    {
      "text": "%pyspark\n\n#\n# fold with 0 Example\n#\ndata = [1,2,3,4,5]\n\nrdd_1 = spark.sparkContext.parallelize(data, 2)\n\nprint(\"Partitions : {}\".format(rdd_1.getNumPartitions()))\nprint(\"Total : {}\".format(rdd_1.fold(0, lambda x, y : x + y)))\nprint(\"Total with init value 2 : {}\".format(rdd_1.fold(0, lambda x, y : x + y)))\nprint(\"Min : {}\".format(rdd_1.fold(0, lambda x, y : min(x, y) )))\nprint(\"Max : {}\".format(rdd_1.fold(0, lambda x, y : max(x, y) )))\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T10:29:38+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Partitions : 2\nTotal : 15\nTotal with init value 2 : 15\nMin : 0\nMax : 5\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=4",
              "$$hashKey": "object:832"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=5",
              "$$hashKey": "object:833"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=6",
              "$$hashKey": "object:834"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=7",
              "$$hashKey": "object:835"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638959080603_1932100621",
      "id": "paragraph_1638440302410_2060859008",
      "dateCreated": "2021-12-08T10:24:40+0000",
      "dateStarted": "2021-12-08T10:29:38+0000",
      "dateFinished": "2021-12-08T10:29:38+0000",
      "status": "FINISHED",
      "$$hashKey": "object:192"
    },
    {
      "text": "%pyspark\n\n#\n# fold with 2 Example\n#\ndata = [1,2,3,4,5]\n\nrdd_1 = spark.sparkContext.parallelize(data, 2)\n\nprint(\"Partitions : {}\".format(rdd_1.getNumPartitions()))\nprint(\"Total : {}\".format(rdd_1.fold(2, add)))\nprint(\"Total with init value 2 : {}\".format(rdd_1.fold(2, lambda x, y : x + y)))\nprint(\"Min : {}\".format(rdd_1.fold(2, lambda x, y : min(x, y) )))\nprint(\"Max : {}\".format(rdd_1.fold(2, lambda x, y : max(x, y) )))\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T10:29:43+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Partitions : 2\n2+5=7\n7+14=21\nTotal : 21\nTotal with init value 2 : 21\nMin : 1\nMax : 5\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=8",
              "$$hashKey": "object:893"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=9",
              "$$hashKey": "object:894"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=10",
              "$$hashKey": "object:895"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=11",
              "$$hashKey": "object:896"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638959080603_1987442910",
      "id": "paragraph_1638962626048_985387068",
      "dateCreated": "2021-12-08T10:24:40+0000",
      "dateStarted": "2021-12-08T10:29:43+0000",
      "dateFinished": "2021-12-08T10:29:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:193"
    },
    {
      "text": "%pyspark\n\n#\n# Broadcast Variables\n#\n\nbroadcastVar = sc.broadcast([1, 2, 3])\nprint(\"{}\".format(broadcastVar.value))\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T10:47:04+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[1, 2, 3]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638959080603_1765484039",
      "id": "paragraph_1638962691917_1511648622",
      "dateCreated": "2021-12-08T10:24:40+0000",
      "dateStarted": "2021-12-08T10:47:04+0000",
      "dateFinished": "2021-12-08T10:47:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:194"
    },
    {
      "text": "%pyspark\n\n#\n# Accumulators\n#\n\naccum = sc.accumulator(0)\n\nacc_rd = sc.parallelize(data,4)\nacc_rd.foreach(lambda x : accum.add(x))\n\nprint(\"{}\".format(accum.value))\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-08T10:51:29+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "15\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=12",
              "$$hashKey": "object:994"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638960380810_818547062",
      "id": "paragraph_1638960380810_818547062",
      "dateCreated": "2021-12-08T10:46:20+0000",
      "dateStarted": "2021-12-08T10:51:04+0000",
      "dateFinished": "2021-12-08T10:51:05+0000",
      "status": "FINISHED",
      "$$hashKey": "object:195"
    },
    {
      "text": "%pyspark\n\n#\n#\n#\n\npremierRDD = sc.parallelize([\n (\"Arsenal\", \"2014–2015\", 75), (\"Arsenal\", \"2015–2016\", 71), (\"Arsenal\", \"2016–2017\", 75), (\"Arsenal\", \"2017–2018\", 63),\n (\"Chelsea\", \"2014–2015\", 87), (\"Chelsea\", \"2015–2016\", 50), (\"Chelsea\", \"2016–2017\", 93), (\"Chelsea\", \"2017–2018\", 70), \n (\"Liverpool\", \"2014–2015\", 62), (\"Liverpool\", \"2015–2016\", 60), (\"Liverpool\", \"2016–2017\", 76), (\"Liverpool\", \"2017–2018\", 75),\n (\"M. City\", \"2014–2015\", 79), (\"M. City\", \"2015–2016\", 66), (\"M. City\", \"2016–2017\", 78), (\"M. City\", \"2017–2018\", 100), \n (\"M. United\", \"2014–2015\", 70), (\"M. United\", \"2015–2016\", 66), (\"M. United\", \"2016–2017\", 69), (\"M. United\", \"2017–2018\", 81) \n ])\n\n# for i in premierRDD.collect(): print(i)\n\n\n#\n#\n#\npremierMap = premierRDD.map(lambda t: (t[0], (t[1], t[2])))\npremierMap.first()\n\n\n#\n# find the maximum points of each teams.\n#\n\n#\nprint(\" Max\")\nprint(\" #\" * 30)\ndef seqFunc(acc, teams):\n if(acc[1] > teams[1]):\n    return acc \n else: \n    return teams\n\n#\ndef combFunc(acc1, acc2):\n if(acc1[1] > acc2[1]):\n    return acc1 \n else:\n    return acc2\n    \npremierMax = premierMap.aggregateByKey(('', 0), seqFunc, combFunc)\nfor i in premierMax.collect(): print(i)\n\nprint(\" #\" * 30)\n\n#\n# Let’s find total points and then start to describe zeroValue\n#\nprint(\"  \" * 30)\nprint(\" Aggregate Sum\")\nprint(\" #\" * 30)\n\ntotalSeqFunc = (lambda x, y: (x[0] + y[1], x[1] + 1))\ntotalCombFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n\npremierAgg = premierMap.aggregateByKey((0,0), totalSeqFunc, totalCombFunc)\nfor i in premierAgg.collect(): print(i)\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-09T04:04:42+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=41",
              "$$hashKey": "object:2187"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=42",
              "$$hashKey": "object:2188"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=43",
              "$$hashKey": "object:2189"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638960664997_1447366215",
      "id": "paragraph_1638960664997_1447366215",
      "dateCreated": "2021-12-08T10:51:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:196",
      "dateFinished": "2021-12-09T04:04:43+0000",
      "dateStarted": "2021-12-09T04:04:42+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": " Max\n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n('M. United', ('2017–2018', 81))\n('Liverpool', ('2016–2017', 76))\n('M. City', ('2017–2018', 100))\n('Chelsea', ('2016–2017', 93))\n('Arsenal', ('2016–2017', 75))\n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n                                                            \n Aggregate Sum\n # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n('M. United', (286, 4))\n('Liverpool', (273, 4))\n('M. City', (323, 4))\n('Chelsea', (300, 4))\n('Arsenal', (284, 4))\n"
          }
        ]
      }
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-09T03:37:56+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639021076440_359626603",
      "id": "paragraph_1639021076440_359626603",
      "dateCreated": "2021-12-09T03:37:56+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:1216"
    }
  ],
  "name": "pyspark-local-rdd-actions",
  "id": "2GR44QU28",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/pyspark-local-rdd-actions"
}